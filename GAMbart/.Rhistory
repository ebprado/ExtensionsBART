curve(dcmp(x, 2, 3), 0, 10, add=TRUE, col=4)
curve(dcmp(x, 2, 1), 0, 10)
curve(dcmp(x, 2, 3), 0, 10, add=TRUE, col=2)
curve(dcmp(x, 2, 1.5), 0, 10, add=TRUE, col=2)
curve(dcmp(x, 2, 1), 0, 10)
curve(dcmp(x, 2, 1.5), 0, 10, add=TRUE, col=2)
curve(dcmp(x, 2, 1.5), 0, 10)
curve(dcmp(x, 2, 1.5), 0, 10, col=2)
curve(dcmp(x, 2, 1), 0, 10, add=TRUE)
curve(dcmp(x, 2, 3), 0, 10, col=3)
curve(dcmp(x, 2, 1.5), 0, 10, col=2, col=TRUE)
curve(dcmp(x, 2, 3), 0, 10, col=3)
curve(dcmp(x, 2, 1.5), 0, 10, col=2, add=TRUE)
curve(dcmp(x, 2, 1), 0, 10, add=TRUE)
curve(dcmp(x, 2, 5), 0, 10, col=4)
curve(dcmp(x, 2, 3), 0, 10, col=3, add=TRUE)
curve(dcmp(x, 2, 1.5), 0, 10, col=2, add=TRUE)
curve(dcmp(x, 2, 1), 0, 10, add=TRUE)
curve(dcmp(x, 2, 10), 0, 10, col=5)
curve(dcmp(x, 2, 5), 0, 10, col=4, add=TRUE)
curve(dcmp(x, 2, 20), 0, 10, col=5)
curve(dcmp(x, 2, 5), 0, 10, col=4, add=TRUE)
curve(dcmp(x, 2, 3), 0, 10, col=3, add=TRUE)
curve(dcmp(x, 2, 1.5), 0, 10, col=2, add=TRUE)
curve(dcmp(x, 2, 1), 0, 10, add=TRUE)
## COM-Poisson
library(COMPoissonReg)
curve(dcmp(x, 2, 1), 0, 10)
curve(dcmp(x, 2, 0.5), 0, 10, add=TRUE, col=2)
curve(dcmp(x, 2, 0.3), 0, 10, add=TRUE, col=3)
curve(dcmp(x, 2, 20), 0, 10, col=5)
curve(dcmp(x, 2, 20, log=TRUE), 0, 10, col=5)
curve(dcmp(x, 2, 5, log=TRUE), 0, 10, col=4, add=TRUE)
curve(dcmp(x, 2, 3, log=TRUE), 0, 10, col=3, add=TRUE)
curve(dcmp(x, 2, 1.5, log=TRUE), 0, 10, col=2, add=TRUE)
curve(dcmp(x, 2, 1, log=TRUE), 0, 10, add=TRUE)
dcmp(0:10, 2, 5, log=TRUE)
gambart.fit2 = gam_bart(x,y, str='original', penalty_add_cov = TRUE, penalty_lambda = 0.0002, one_var_per_tree = FALSE, nburn = 100, npost = 100)
plot(y, apply(gambart.fit2$y_hat,2,mean)); abline(0,1, col=2)
cor(y, apply(gambart.fit2$y_hat,2,mean))
gambart.fit2$trees[[100]]
set.seed(001)
gambart.fit2 = gam_bart(x,y, str='original', penalty_add_cov = TRUE, penalty_lambda = 2, one_var_per_tree = FALSE, nburn = 100, npost = 100)
plot(y, apply(gambart.fit2$y_hat,2,mean)); abline(0,1, col=2)
cor(y, apply(gambart.fit2$y_hat,2,mean))
gambart.fit2$trees[[100]]
w = rnorm(n, 0,1)
y = rbinom(n,1,0.5)
y==0
n = 100
w = rnorm(n, 0,1)
y = rbinom(n,1,0.5)
z = rep(NA,n)
z
z = ifelse(w[y==0] < 0, w, 0)
z
z = rep(NA,n)
z
z[y==0] = ifelse(w[y==0] < 0, w, 0)
z
n = 100
w = rnorm(n, 0,1)
y = rbinom(n,1,0.5)
z = rep(NA,n)
z[y==0] = ifelse(w[y==0] < 0, w, 0)
z[y==1] = ifelse(w[y==1] > 0, w, 0)
hist(z)
z
n = 100
w = rnorm(n, 0,1)
y = rbinom(n,1,0.5)
z = rep(NA,n)
z[y==0] = ifelse(w[y==0] < 0, w, 0)
z[y==1] = ifelse(w[y==1] > 0, w, 0)
hist(z)
library(truncnorm)
length(y==0)
y==0
sum(y==0)
ny1 = sum(y==1)
ny1
n = 100
w = rnorm(n, 0,1)
y = rbinom(n,1,0.5)
z = rep(NA,n)
ny0 = sum(y==0)
ny1 = sum(y==1)
z[y==0] = rtruncnorm(ny0, a = -Inf, b=0, mean = 0, 1)
z[y==1] = rtruncnorm(ny1, a = 0, b=Inf, mean = 0, 1)
hist(z)
z
y==0
n = 100
w = rnorm(n, 0,1)
y = rbinom(n,1,0.5)
z = rep(NA,n)
z[y==0] = pmin(w[y==0], 0)
z[y==1] = pmax(w[y==1], 0)
hist(z)
w
hist(w)
n = 100
w = rnorm(n, 0,1)
y = rbinom(n,1,0.5)
z = rep(NA,n)
ny0 = sum(y==0)
ny1 = sum(y==1)
z[y==0] = rtruncnorm(ny0, a = -Inf, b=0, mean = 0, 1)
z[y==1] = rtruncnorm(ny1, a = 0, b=Inf, mean = 0, 1)
hist(z)
n = 100
n = 100
w = rnorm(n, 0,1)
y = rbinom(n,1,0.5)
z = rep(NA,n)
ny0 = sum(y==0)
ny1 = sum(y==1)
z[y==0] = rtruncnorm(ny0, a = -Inf, b=0, mean = w[y==0], 1)
z[y==1] = rtruncnorm(ny1, a = 0, b=Inf, mean = w[y==1], 1)
hist(z)
z
n = 100
fx = rnorm(n, 0, 2)
w = rnorm(n, fx,1)
n = 100
fx = rnorm(n, 0, 2)
w = rnorm(n, fx,1)
y = rbinom(n,1,0.5)
z = rep(NA,n)
z[y==0] = ifelse(w[y==0] < 0, w, 0)
z[y==1] = ifelse(w[y==1] > 0, w, 0)
hist(z)
z
w[y==0]
n = 100
fx = rnorm(n, 0, 2)
w = rnorm(n, fx,1)
y = rbinom(n,1,0.5)
z = rep(NA,n)
z[y==0] = ifelse(w[y==0] < 0, w[y==0], 0)
z[y==1] = ifelse(w[y==1] > 0, w[y==1], 0)
hist(z)
n = 100
fx = rnorm(n, 0, 2)
w = rnorm(n, fx,1)
y = rbinom(n,1,0.5)
z = rep(NA,n)
y==0
ifelse(w[y==0] < 0, w[y==0], 0)
sum(y==0)
z = rep(NA,n)
z[y==0] = ifelse(w[y==0] < 0, w[y==0], 0)
z
ifelse(w[y==0] < 0, w[y==0], 0)
z[y==1] = ifelse(w[y==1] > 0, w[y==1], 0)
hist(z)
n = 100
fx = rnorm(n, 0, 2)
w = rnorm(n, fx,1)
y = rbinom(n,1,0.5)
z = rep(NA,n)
ny0 = sum(y==0)
set.seed(001)
n = 100
fx = rnorm(n, 0, 2)
w = rnorm(n, fx,1)
y = rbinom(n,1,0.5)
z = rep(NA,n)
z[y==0] = ifelse(w[y==0] < 0, w[y==0], 0)
z[y==1] = ifelse(w[y==1] > 0, w[y==1], 0)
hist(z)
n = 100
set.seed(001)
n = 100
fx = rnorm(n, 0, 2)
w = rnorm(n, fx,1)
y = rbinom(n,1,0.5)
z = rep(NA,n)
z[y==0] = ifelse(w[y==0] < 0, w[y==0], 0)
z[y==1] = ifelse(w[y==1] > 0, w[y==1], 0)
hist(z)
set.seed(001)
n = 100
fx = rnorm(n, 0, 2)
w = rnorm(n, fx,1)
y = rbinom(n,1,0.5)
z = rep(NA,n)
z[y==0] = ifelse(w[y==0] < 0, w[y==0], 0)
z[y==1] = ifelse(w[y==1] > 0, w[y==1], 0)
hist(z)
n = 100
z = rep(NA,n)
ny0 = sum(y==0)
ny1 = sum(y==1)
z[y==0] = rtruncnorm(ny0, a = -Inf, b=0, mean = fx[y==0], 1)
z[y==1] = rtruncnorm(ny1, a = 0, b=Inf, mean = fx[y==1], 1)
hist(z)
z
n = 100
fx = rnorm(n, 0, 2)
w = rnorm(n, fx,1)
y = rbinom(n,1,0.5)
set.seed(001)
n = 100
fx = rnorm(n, 0, 2)
w = rnorm(n, fx,1)
y = rbinom(n,1,0.5)
z = rep(NA,n)
z[y==0] = ifelse(w[y==0] < 0, w[y==0], 0)
z[y==1] = ifelse(w[y==1] > 0, w[y==1], 0)
hist(z)
z = rep(NA,n)
ny0 = sum(y==0)
ny1 = sum(y==1)
z[y==0] = rtruncnorm(ny0, a = -Inf, b=0, mean = fx[y==0], 1)
z[y==1] = rtruncnorm(ny1, a = 0, b=Inf, mean = fx[y==1], 1)
hist(z)
z = rep(NA,n)
z = rep(NA,n)
z = rep(NA,n)
z[y==0] = pmin(w[y==0], 0)
z[y==1] = pmax(w[y==1], 0)
hist(z)
z = rep(NA,n)
z[y==0] = ifelse(w[y==0] < 0, w[y==0], 0)
z[y==1] = ifelse(w[y==1] > 0, w[y==1], 0)
hist(z)
z = rep(NA,n)
ny0 = sum(y==0)
ny1 = sum(y==1)
z[y==0] = rtruncnorm(ny0, a = -Inf, b=0, mean = fx[y==0], 1)
z[y==1] = rtruncnorm(ny1, a = 0, b=Inf, mean = fx[y==1], 1)
hist(z)
z
set.seed(001)
n = 100
fx = rnorm(n, 0, 2)
w = rnorm(n, fx,1)
y = rbinom(n,1,0.5)
z = rep(NA,n)
z[y==0] = ifelse(w[y==0] < 0, w[y==0], 0)
z[y==1] = ifelse(w[y==1] > 0, w[y==1], 0)
hist(z)
z
z3 = rep(NA,n)
z3[y==0] = pmin(w[y==0], 0)
z3[y==1] = pmax(w[y==1], 0)
hist(z3)
z2 = rep(NA,n)
ny0 = sum(y==0)
ny1 = sum(y==1)
z2[y==0] = rtruncnorm(ny0, a = -Inf, b=0, mean = fx[y==0], 1)
z2[y==1] = rtruncnorm(ny1, a = 0, b=Inf, mean = fx[y==1], 1)
hist(z2)
z2
# The way the referee understood our algorithm
set.seed(001)
n = 100
fx = rnorm(n, 0, 2)
w = rnorm(n, fx,1)
y = rbinom(n,1,0.5)
z = rep(NA,n)
z[y==0] = ifelse(w[y==0] < 0, w[y==0], 0)
z[y==1] = ifelse(w[y==1] > 0, w[y==1], 0)
hist(z)
set.seed(001)
n = 100
fx = rnorm(n, 0, 2) # sum of the trees
w = rnorm(n, fx, 1)
y = rbinom(n, 1, 0.5)
z = rep(NA,n)
z[y==0] = ifelse(w[y==0] < 0, w[y==0], 0)
z[y==1] = ifelse(w[y==1] > 0, w[y==1], 0)
hist(z)
z
z2 = rep(NA,n)
z2[y==0] = pmin(w[y==0], 0)
z2[y==1] = pmax(w[y==1], 0)
hist(z2)
z2
z3 = rep(NA,n)
ny0 = sum(y==0)
ny1 = sum(y==1)
z3[y==0] = rtruncnorm(ny0, a = -Inf, b=0, mean = fx[y==0], 1)
z3[y==1] = rtruncnorm(ny1, a = 0, b=Inf, mean = fx[y==1], 1)
hist(z3)
z3
z3 = rep(NA,n)
ny0 = sum(y==0)
z3
z3 = rep(NA,n)
ny0 = sum(y==0)
ny1 = sum(y==1)
z3[y==0] = rtruncnorm(ny0, a = -Inf, b=0, mean = fx[y==0], 1)
z3[y==0] = rtruncnorm(ny0, a = -Inf, b=0, mean = fx[y==0], 1)
z3[y==1] = rtruncnorm(ny1, a = 0, b=Inf, mean = fx[y==1], 1)
hist(z3)
z3
# The way the referee understood our algorithm
set.seed(001)
n = 100
fx = rnorm(n, 0, 2) # sum of the trees
w = rnorm(n, fx, 1)
y = rbinom(n, 1, 0.5)
z = rep(NA,n)
z[y==0] = ifelse(w[y==0] < 0, w[y==0], 0)
z[y==1] = ifelse(w[y==1] > 0, w[y==1], 0)
hist(z)
## The way is implemented in R
z2 = rep(NA,n)
z2[y==0] = pmin(w[y==0], 0)
z2[y==1] = pmax(w[y==1], 0)
hist(z2)
### Albert and Chib (1993)
z3 = rep(NA,n)
ny0 = sum(y==0)
ny1 = sum(y==1)
z3[y==0] = rtruncnorm(ny0, a = -Inf, b=0, mean = fx[y==0], 1)
z3[y==1] = rtruncnorm(ny1, a = 0, b=Inf, mean = fx[y==1], 1)
hist(z3)
z3
# The way the referee understood our algorithm
set.seed(001)
n = 100
fx = rnorm(n, 0, 2) # sum of the trees
w = rnorm(n, fx, 1)
y = rbinom(n, 1, 0.5)
z = rep(NA,n)
z[y==0] = ifelse(w[y==0] < 0, w[y==0], 0)
z[y==1] = ifelse(w[y==1] > 0, w[y==1], 0)
hist(z)
## The way is implemented in R
z2 = rep(NA,n)
z2[y==0] = pmin(w[y==0], 0)
z2[y==1] = pmax(w[y==1], 0)
hist(z2)
### Albert and Chib (1993)
z3 = rep(NA,n)
ny0 = sum(y==0)
ny1 = sum(y==1)
z3[y==0] = rtruncnorm(ny0, a = -Inf, b=0, mean = fx[y==0], 1)
z3[y==1] = rtruncnorm(ny1, a = 0, b=Inf, mean = fx[y==1], 1)
hist(z3)
z3
fx[y==0]
ny0
install()
library(devtools)
install()
load_all()
document()
check()
# The way the referee understood our algorithm
set.seed(001)
n = 100
fx = rnorm(n, 0, 2) # sum of the trees
w = rnorm(n, fx, 1)
y = rbinom(n, 1, 0.5)
z = rep(NA,n)
z[y==0] = ifelse(w[y==0] < 0, w[y==0], 0)
z[y==1] = ifelse(w[y==1] > 0, w[y==1], 0)
hist(z)
z
z2 = rep(NA,n)
z2[y==0] = pmin(w[y==0], 0)
z2[y==1] = pmax(w[y==1], 0)
hist(z2)
z3 = rep(NA,n)
ny0 = sum(y==0)
ny1 = sum(y==1)
z3[y==0] = rtruncnorm(ny0, a = -Inf, b=0, mean = fx[y==0], 1)
z3[y==1] = rtruncnorm(ny1, a = 0, b=Inf, mean = fx[y==1], 1)
hist(z3)
z3
install_github("ebprado/MOTR-BART/GAMbart")
library(devtools)
install_github("ebprado/MOTR-BART/GAMbart")
library(GAMbart)
# Simulate a Friedman data set
friedman_data = function(n, num_cov, sd_error){
x = matrix(runif(n*num_cov),n,num_cov)
y = 10*sin(pi*x[,1]*x[,2]) + 20*(x[,3]-.5)^2+10*x[,4]+5*x[,5] + rnorm(n, sd=sd_error)
return(list(y = y,
x = x))
}
# Training data
data = friedman_data(200, 10, 1)
y = data$y
x = data$x
# Test data
data_test = friedman_data(100, 10, 1)
y.test = data_test$y
x.test = data_test$x
# Run GAM-BART
set.seed(99)
fit.gam.bart = gam_bart(x, y, str = 'original', ntrees = 10, nburn = 1000, npost = 100, df=5, dg=3, ancestors = FALSE, penalty = 'ridge')
fit.gam.bart = gam_bart(x, y, str = 'original', ntrees = 10, nburn = 100, npost = 100, df=5, dg=3, ancestors = FALSE, penalty = 'ridge')
plot(y,apply(fit.gam.bart$y_hat,2,mean));abline(0,1)
fit.gam.bart$trees[[100]]
install()
load_all()
document()
check()
build()
library(devtools)
install()
load_all()
document()
check()
build()
library(ggplot2)
library(tidyverse)
# Where the file containing the consolidated results is
save_file = "/Users/estevaoprado/Documents/GitHub/AMBARTI/04_simulation/results/"
# Load the consolidated results
load(paste(save_file, '00_results_consolidated.RData', sep=''))
# Some preprocessing
tab = save_results
tab$id = factor(tab$id, levels = c('classical AMMI', 'Bayesian AMMI (postproc)', 'Bayesian AMMI (NO postproc)', 'AMBARTI'),
labels = c('AMMI', 'B-AMMI (postproc)', 'B-AMMI (no postproc)', 'AMBARTI'))
tab$Q = factor(sapply(strsplit(as.character(tab$lambda), split = ' '), function(x) length(x)), levels = c('1','2','3'), labels = c('Q = 1', 'Q = 2', 'Q = 3'))
tab$I = factor(tab$I, levels=c('10'), labels=c('I = 10'))
tab$J = factor(tab$J, levels=c('10'), labels=c('J = 10'))
tab$lambda = as.factor(tab$lambda)
tab$sa = as.factor(tab$sa)
tab$sb = as.factor(tab$sb)
tab$sy = as.factor(tab$sy)
# Generate plots
myplot <- function(varA, varB, varC){
if (varA == 'sa'){aux = expression(sigma[alpha])}
if (varA == 'sb'){aux = expression(sigma[beta])}
if(varB=='rrmse_alpha'){varC = expression("RRMSE - "~alpha[i])}
if(varB=='rrmse_beta'){varC = expression("RRMSE - "~beta[j])}
if(varB=='lambda_rrmse'){varC = expression("RRMSE - "~lambda[q])}
if(varB=='gamma_rrmse'){varC = expression("RRMSE - "~gamma[iq])}
if(varB=='delta_rrmse'){varC = expression("RRMSE - "~delta[jq])}
tab %>%
ggplot(aes_string(x = varA , y = varB, colour='id')) +
geom_boxplot(outlier.shape = 1) +
labs(x = aux,
title = varC,
colour = '',
y = 'RMSE') +
theme_bw(base_size = 12) +
theme(plot.title = element_text(size = 15, hjust = 0.5),
legend.position = 'bottom',
panel.grid.major = element_blank(),
panel.grid.minor = element_blank()) +
facet_wrap(~ Q, scales='free', nrow=1) +
# facet_grid(I ~ Q) +
labs(colour='') +
guides(col = guide_legend(nrow=2))
}
myplot('sb','y_test_rmse', 'RMSE - y test')
myplot('sa','y_test_rmse', 'RMSE - y test')
myplot('sa','rrmse_alpha', 'RRMSE - alpha')
myplot('sb','rrmse_beta',  'RRMSE - beta')
myplot('sb','lambda_rrmse', 'RRMSE - lambda')
myplot('sb','gamma_rrmse', 'RRMSE - gamma')
myplot('sb','delta_rrmse', 'RRMSE - delta')
choose(20, 1:10)
choose(20, 2:10)
sum(choose(20, 2:10))
sample(2:10, 1)
sample(1:20, 3)
sample(1:20, sample(2:10,1))
ne = 50
sample(1:ne, sample(2:(ne/2),1))
1:ne
ne = 5
ne = 5
ne = 10
choose(ne, 2:floor(ne/2))
pesofinal = peso1/sum(peso1)
pesofinal
pesofinal = peso1/sum(peso1)
pesofinal
sample(1:ne, sample(2:(ne/2),1, prob = pesofinal))
ne = 10
peso1 = choose(ne, 2:floor(ne/2))
pesofinal = peso1/sum(peso1)
sample(1:ne, sample(2:(ne/2),1, prob = pesofinal))
load("~/Documents/GitHub/AMBARTI/04_simulation/results/I10J10sa1sb1sy1L1012r10_data.RData")
dbinom(0:10,size = 10, prob = 0.01)
sum(dbinom(1:10,size = 10, prob = 0.01))
library(dplyr)
library(mgcv)
set.seed(2) ## simulate some data...
dat <- gamSim(1,n=400,dist="normal",scale=2)
b <- gam(y~s(x0) + s(x1) + s(x2), data=dat)
summary(b)
yscale = scale(dat[,'y'])
db = as.data.frame(cbind(y = dat[, 'y'],
x0 = dat[, 'x0'],
x1 = dat[, 'x1'],
x2 = dat[, 'x2'],
x3 = dat[, 'x3'],
yhat = b$fitted.values,
res = b$residuals))
dat_test = as.data.frame(cbind(x0 = rep(0, nrow(dat)),
x1 = rep(0, nrow(dat)),
x2 = sort(dat[,'x2']),
x3 = rep(0, nrow(dat))))
plot(b,pages=0, residuals = TRUE, se = FALSE, main = 'plot from package', select = 3)
pred_test = predict.gam(b, dat_test)
pred_test = predict.gam(b, dat_test)
aux_pred = sum(predict.gam(b, dat_test, type='terms')[1,1:2])
final_pred = pred_test - (attr(yscale, 'scaled:center') + aux_pred)
lines(dat_test[,'x2'], final_pred, col=2, lty=2);
points(dat_test[,'x2'], final_pred + (db %>% arrange(x2))$res, col=2, lty=2);
plot(dat[,'x2'], dat[,'y'])
lines(dat_test[,'x2'], pred_test - aux_pred, col=2);
